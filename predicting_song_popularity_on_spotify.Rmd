---
title: 'Exploratory Data Analysis and Model Building'
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error=TRUE)

library(tidyverse)
library(vioplot)
library(corrplot)
library(naniar)
library(GGally)
library(mice)
library(car)
library(cowplot)
library(tidymodels)
library(finetune)
library(rsample)
library(knitr)
library(DALEXtra)
tidymodels_prefer()


#Read in unique data
data <- read.csv("spotify_unique_data.csv")
```


#### Missing Data & Multivariate Imputation by Chained Equations

To prepare the data for initial exploratory analysis, data set elements are output to assess data structure and identify duplicated rows. Moreover, the column binomial is excluded as a feature and the explanatory variables Key, Audio Mode, and Time signature are transformed to factor which best reflects the data type of the audio features. 


```{r data structure dimensions}
paste0("The data on Spotify music has ", nrow(data), " rows and ", ncol(data), " columns.")
```

**Variables**

* Song Name: This is the name of the song.

* **Song Popularity**: This is the popularity of the song, quantified by a value between 0 and 100. A
value of 0 means that the song is not popular, and a value of 100 means that the song is extremely
popular.

* **Duration ms**: This is the duration of the song in milliseconds.

* **Acousticness**: This is a confidence measure from 0.0 to 1.0 of whether the song is acoustic. 1.0
represents high confidence that the song is acoustic, and vice versa.

* **Danceability**: This describes how suitable a song is for dancing based on a combination of musical
elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is low
danceability and 1.0 is high danceability.

* **Energy**: This is a measure of the song’s energy from 0.0 to 1.0 and represents a perceptual measure
of intensity and activity. Typically, energetic songs feel fast, loud, and noisy.

* **Instrumentalness**: This predicts whether a song contains no vocals. “Ooh” and “aah” sounds are
treated as instrumental in this context. Rap or spoken word songs are clearly “vocal”. The closer the
instrumentalness value is to 1.0, the greater likelihood the song contains no vocal content. Values above
0.5 are intended to represent instrumental songs, but confidence is higher as the value approaches 1.0.

* **Key**: This is the key the song is in. Integers map to pitches using standard Pitch Class notation.

* **Liveness**: This detects the presence of an audience in the recording. Higher liveness values represent
an increased probability that the song was performed live. A value above 0.8 provides strong likelihood
that the song is live.

* **Loudness**: The overall loudness of a song in decibels (dB). Loudness is the quality of a sound that
is the primary psychological correlate of physical strength (amplitude). Loudness values are averaged
across the entire song. Values typically range between -60 and 0 dB.

* **Audio Mode**: This indicates the modality (major or minor) of a song, the type of scale from which
its melodic content is derived. Major is represented by 1 and minor is 0.

* **Speechiness**: This detects the presence of spoken words in a song. The more exclusively speech-like
the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value.

* **Tempo**: This is the beats per minute (BPM) which is the speed or pace of a given piece and derives
directly from the average beat duration.

* **Time Signature**: The time signature is a notational convention to specify how many beats are in
each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of “3/4”, to
“7/4”.

* **Audio Valence**: This is a measure of the perceived happiness of the song. Audio valence is a measure
from 0.0 to 1.0, the closer to 1.0, the more positive (or happier) a song is.

```{r data structure unique data types}
paste0("There are ", length(unique(sapply(data, class))), " unique data types.")
```
```{r data structure data types}
str(data)
```

```{r data structure changes}
data <- data |> 
  select(-binomial)

data <- mutate_at(data, vars(Key, Audio_mode, Time_signature),
                    as.factor)
```


To clean the data set, duplicated rows are identified and removed, measurement or data entry errors are screened and missing values are tabulated to capture the extent of missing data across the variables. 


```{r duplicated rows count}
duplicated.rows <- data[duplicated(data), ]

paste0("The data set has ", nrow(duplicated.rows), " duplicated rows.")
```
```{r duplicated rows removed}
data.cleaned <- distinct(data)

paste0("The dimensions of the cleaned data set are ", nrow(data.cleaned), " rows and ", ncol(data.cleaned), " columns.")
```


To assess and remedy variables with 'bad' data characteristics which encompasses inaccuracy, inconsistency, incompleteness, invalidity or non-uniformity, summary statistics for each column are diagnosed for errors and inconsistencies. 

The appropriate ranges and units of measure for the 15 attributes include:

**Song Popularity**: 0 - 100 (integer)

**Duration ms**: 0+ (integer)

**Acousticness**: 0.0 - 1.0 (numeric)

**Danceability**: 0.0 - 1.0 (numeric)

**Energy**: 0.0 - 1.0 (numeric)

**Instrumentalness**: 0.0 - 1.0 (numeric)

**Key**: 0, 1, 2, 7 and 9 (factor)

**Liveness**: 0.0 - 1.0 (numeric)

**Loudness**: -60 - 0 (numeric)

**Audio Mode**: 0 and 1 (factor)

**Speechiness**: 0.0 - 1.0 (numeric)

**Tempo**: 0+ (integer)

**Time Signature**: 3, 4, 5, 6 and 7 (factor)

**Audio Valence**: 0.0 - 1.0 (numeric)

```{r loctions measures summary}
summary(data.cleaned)
```

```{r row count of missing values in the columns Duration ms, Acousticness, Loudness, Tempo and Time Signature}
measurement.errors <- data.cleaned |>
  select(Duration_ms, Acousticness, Loudness, Tempo, Time_signature)

row.count <- n_case_miss(measurement.errors)

paste0("There are ", row.count, " rows with measurement or data entry errors across the features Duration ms, Acousticness, Loudness, Tempo and Time Signature.")
```


A summary of the data reveals that there are measurement or data entry errors across the variables Duration ms (685 observations), Acousticness (639 observations), Loudness (666 observations), Tempo (636 observations) and Time Signature (647 observations). Of the 12,316 songs there are 3,273 distinct data points with values that are not consistent with the specified measurement ranges. To remedy the inaccurate measurements, the values are transformed to NA values to further determine whether the data set's missing values are missing at random (MAR), missing completely at random (MCAR) or not missing at random (NMAR). The missing data mechanisms describe possible relationships between the observed data points, unobserved features and missing values. Thereafter, the appropriate imputation method will be selected to best prepare the entire data set for statistical machine learning. 


```{r measurement or data entry errors for the columns Duration ms, Acousticness, Loudness, Tempo and Time Signature}
data.cleaned$Duration_ms[data.cleaned$Duration_ms < 0] <- NA

data.cleaned$Acousticness[data.cleaned$Acousticness < 0 | data.cleaned$Acousticness > 1] <- NA

data.cleaned$Loudness[data.cleaned$Loudness > 1] <- NA

data.cleaned$Tempo[data.cleaned$Tempo < 0] <- NA

data.cleaned$Time_signature[data.cleaned$Time_signature == 0 | data.cleaned$Time_signature == 1] <- NA
```


The 5 observed columns had varying counts of inaccurate measurements.

 **Duration_ms**: 5
 
 **Acousticness**: 128
 
 **Loudness**: 1
 
 **Tempo**: 14
 
 **Time Signature**: 61
 
 209 data points were cumulatively amended. 


```{r descriptive statistics for the columns Duration ms, Acousticness, Loudness, Tempo and Time Signature}
measurement.errors2 <- data.cleaned |>
  select(Duration_ms, Acousticness, Loudness, Tempo, Time_signature)

summary(measurement.errors2)
```

```{r count of missing values}
paste0("There are ", sum(is.na(data.cleaned)), " missing values across the data set.")
```

```{r tabulation of missing values}
missing.data.summary <- miss_var_summary(data.cleaned)

missing.data.summary <- missing.data.summary |>
  rename(
    Variable = variable,
    Count = n_miss,
    Percentage = pct_miss
  )

kable(missing.data.summary)
```


The audio feature with the most missing data points and highest percentage of missing values is Energy. 1051 observations are missing which accounts for 8.53% of the variable. 11 attributes have a relatively similar count and percentage of missing values which range between 620 - 767 (difference of 47 values) and 5.03% - 6.23%  (difference of 1.20%) respectively. 


```{r visual analogue of missing values using miss_var_summary}
gg_miss_var(data.cleaned) +
  ggtitle("Missing Data Summary") +  
  xlab("Variables") +               
  ylab("Count of Missing Values") + 
  theme_minimal() +                  
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  
    plot.title = element_text(hjust = 0.5)               
  ) 
```


Of the 15 variables, 12 have missing values. Song name, Song popularity and Audio mode do not have missing values.  Moreover, the introduction of null values increases the total percentage of missing values to 4.4. 

A geom_miss_upset() plot helpfully visualises observations with missing values in a structured and comparable manor. 

To synthesis the missing data mechanisms, all audio features with missing values are grouped according to song structure and technical elements, production characteristics, and musical feel and perceptual attributes.

**Song structure and technical elements**

* Duration ms: MCAR is the most likely missing data mechanism as all audio released for public consumption typically have a playback time. The missing values are likely randomly distributed across the variable and are unrelated to unobserved features or other data set column's with values. Non-standard audio formats including spoken words or ambient noise may lack full audio analysis. 

* Tempo: MAR is the most likely missing data mechanism as missing values may be due to truncated or malformed meta data. Difficulties may arise from extracting the speed or pace of complex tracks. Other observed variables with values including Energy, Loudness, Acousticness and Daneability could account for the missing data points. 

* Time signature: MAR is the most likely missing data mechanism as the extraction of the notation to specify the count of beats in each bar may likely be difficult from live performance tracks. Other observed variables with likely valid values including Liveness, Acousticness, Energy, Speechiness and Danceability could account for the missing data points.

* Key: MAR is the most likely missing data mechanism as certain tracks that may have dialogue including speeches, podcasts or audio-books unlikely have a recognisable Pitch Class (MNAR). The variables speechiness, Liveness and Instrumentalness are a tentative subgroup that could account for the missing data points (MAR). 

**Production charactersitics**

* Loudness: MCAR or MAR are the most likely missing data mechanisms as particular tracks may solely be dialogue with minimal vocal expressions therefore decibel level averages may not have been calculated (MCAR). However, the variables Acousticness, Energy and Instrumentalness with likely valid data points may account for the missing values in the Loudness column (MAR).

* Acousticness: MAR or MCAR are the most likely missing data mechanisms as the missing values are likely correlated to absent values across the Energy and Loudness columns. However, Acousticness estimates could be derived from a subset of likely valid data points from the aforementioned audio features (MAR). Furthermore, the missing data points could be attributed to songs that were electronically processed. Nonetheless, the variable's incompleteness is likely randomly distributed across the variable moreover, the probability of any particular missing acoustic measure may be unrelated to any given known observation that are NA (MCAR).

* Instrumentalness: MAR or NMAR are the most likely missing data mechanisms as vocal detection algorithms may have had difficulty with certain genres. Systematic missing values may be attributed to particular track vocals lacking distinctive musical elements or heavy modification (MNAR). However, the variables Energy and Acousticness could account for song charactersitics with ambient noise, intensity and activity (MAR).

* Liveness: MAR or NMAR are the most likely missing data mechanism as unobserved attributes may better account for low-volume or studio-processed tracks that were recorded with a live audience (NMAR). However, a subgroup of the data set's attributes including Energy and Danceability could account for the presence of a live audience (MAR).

**Musical feel and perceptual attributes**

* Energy: MAR or NMAR are the most likely missing data mechanisms as missing values are likely associated to attributes with similarly missing data points (NMAR) however sufficient likely valid data points are present to derive Energy estimates. Measures on perceptual intensity and activity are likely tied to Acousticeness and Loudenss as a dependency may be observable between Energy and the aforementioned audio features (MAR). 

* Danceability: MAR or NMAR are the most likely missing data mechanisms as missing values may be systematic. Key and Tempo comparably have missing values across rows with missing Danceability data points (NMAR). However, valid data from the columns Key and Tempo could account for Danceability estimates in addition to the audio features Audio valence and Speechiness (MAR). 

* Speechiness: MNAR is the most likely missing data mechanism as speech detection may be tied to minimally trained algorithms due to insufficient data and unsatisfactory parameters. Moreover, of the 28 subgroups with missing data, Speechiness composes 11 of the subsets. 

* Audio valence: NMAR and MAR are the most likely missing data mechanisms as the perceived happiness of a song may be difficult to detect for tracks that have ambiguous tones or an absent value in the Key column. Furthermore, missing values may be correlated to absent energy measures (NMAR). Energy, Audio Mode and Key likely account for the production effects that determine the emotional atmosphere of a track therefore missing value estimates could be calculated (MAR).


```{r upset plot across all features}

gg_miss_upset(data.cleaned, nsets = n_var_miss(data.cleaned)) 

```


geom_miss_point() plots are a useful approach to further explore the three missing data mechanisms (MCAR, MAR, NMAR) and distribution of all the observations for each audio feature.

**Duration ms**

Majority of the songs appear to approximately range between 100,000 and 300,000 milliseconds. There are visible outliers with track lengths that surpass 400,000 milliseconds. Of the observations with missing values, song popularity is concentrated around the scores that roughly lie from 50 to 75. 


```{r missingness of Duration ms by Song Popularity}
ggplot(data.cleaned, aes(x = song_popularity, y = Duration_ms)) +
  theme_minimal() +
  geom_miss_point() +
  labs(
    x = "Song Popularity",
    y = "Milliseconds"
  ) +
  scale_y_continuous(
    breaks = seq(0, max(data.cleaned$Duration_ms, na.rm = TRUE), by = 200000),
    labels = label_comma()
  ) +
  scale_x_continuous(
    breaks = seq(0, max(data.cleaned$song_popularity, na.rm = TRUE), by = 10),
    labels = label_comma()
  ) +
  guides(
    color = guide_legend(title = "Value Type", 
                         title.theme = element_text(family = "sans", 
                                                    face = "bold",
                                                    size = 10))
  ) + 
  ggtitle("Duration ms") +
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black"),
    legend.text = element_text(family = "sans", size = 10)
  )
```


**Acousticness**

Observations appear to be randomly distributed across the Acousticness confidence measure. The bulk of data points along the y-axis roughly range between 0.00 and 0.30 and have a song popularity score in the region of 45 to 85. 


```{r missingness of Acousticness by Song Popularity}

ggplot(data.cleaned, aes(x = song_popularity, y = Acousticness)) +
  theme_minimal() +
  geom_miss_point() +
  labs(
    x = "Song Popularity",
    y = "Confidence Measure"
  ) +
  ggtitle("Acousticness") +
  scale_x_continuous(
    breaks = seq(0, max(data.cleaned$song_popularity, na.rm = TRUE), by = 10),
    labels = label_comma()
  ) +
  scale_y_continuous(
    breaks = seq(0, max(data.cleaned$Acousticness, na.rm = TRUE), by = 0.10),
    labels = label_comma()
  ) +
  guides(
    color = guide_legend(title = "Value Type", 
                         title.theme = element_text(family = "sans", 
                                                    face = "bold",
                                                    size = 10))
  ) + 
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black"),
    legend.text = element_text(family = "sans", size = 10)
  )
```


**Danceability**

Observations for the attribute Danceability are randomly scattered however similarly to the distribution of Acousticness data points, a large portion of values approximately range between 0.00 and 0.30. There is visible sparseness of observations that have a song popularity score that lie from 0 to 30. Moreover, of the observations with missing values, song popularity is concentrated around the scores in the region of 42 to 85. 


```{r missingness of Danceability by Song Popularity}
ggplot(data.cleaned, aes(x = song_popularity, y = Danceability)) +
  theme_minimal() +
  geom_miss_point() +
labs(
  x = "Song Popularity",
  y = "Confidence Measure"
) +
  ggtitle("Danceability") +
  scale_x_continuous(
    breaks = seq(0, max(data.cleaned$song_popularity, na.rm = TRUE), by = 10),
    labels = label_comma()
  ) +
  scale_y_continuous(
    breaks = seq(0, max(data.cleaned$Danceability, na.rm = TRUE), by = 0.10),
    labels = label_comma()
  ) +
  guides(
    color = guide_legend(title = "Value Type", 
                         title.theme = element_text(family = "sans", 
                                                    face = "bold",
                                                    size = 10))
  ) + 
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black"),
    legend.text = element_text(family = "sans", size = 10)
  )
```


**Energy**

A demonstrable concentration of observations is exhibited for observations with an Energy confidence measure in the region of 0.27 and 1.0, and a song popularity score that approximately ranges between 30 and 80. Of the observations with missing values, song popularity is concentrated around the scores that roughly lie from 32 to 79. 


```{r missingness of Energy by Song Popularity}

ggplot(data.cleaned, aes(x = song_popularity, y = Energy)) +
  theme_minimal() +
  geom_miss_point() +
  labs(
    x = "Song Popularity",
    y = "Confidence Measure"
  ) +
  ggtitle("Energy") +
  scale_x_continuous(
    breaks = seq(0, max(data.cleaned$song_popularity, na.rm = TRUE), by = 10),
    labels = label_comma()
  ) +
  scale_y_continuous(
    breaks = seq(0, max(data.cleaned$Energy, na.rm = TRUE), by = 0.10),
    labels = label_comma()
  ) +
  guides(
    color = guide_legend(title = "Value Type", 
                         title.theme = element_text(family = "sans", 
                                                    face = "bold",
                                                    size = 10))
  ) + 
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black"),
    legend.text = element_text(family = "sans", size = 10)
  )
```


**Instrumentalness**

The Instrumentalness data points do not visualise a discernible pattern however a series of observations are visible along 0.00 up to 0.05. The values stretch across the x-axis indicating that song popularity scores are wide spread. There are two distinct outliers with approximate confidence measures of 0.08 and 0.39 with song popularity scores of 92 and 95 respectively. Of the observations with missing values, song popularity is concentrated around the region of 35 and 88. 


```{r missingness of Instrumentalness by Song Popularity}
ggplot(data.cleaned, aes(x = song_popularity, y = Instrumentalness)) +
  theme_minimal() +
  geom_miss_point() +
  labs(
    x = "Song Popularity",
    y = "Confidence Measure"
  ) +
  scale_y_continuous(
    breaks = seq(0, max(data.cleaned$Instrumentalness, na.rm = TRUE), by = 0.10),
    labels = label_comma()
  ) +
  ggtitle("Instrumentalness") +
  scale_x_continuous(
    breaks = seq(0, max(data.cleaned$song_popularity, na.rm = TRUE), by = 10),
    labels = label_comma()
  ) +
  guides(
    color = guide_legend(title = "Value Type", 
                         title.theme = element_text(family = "sans", 
                                                    face = "bold",
                                                    size = 10))
  ) + 
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black"),
    legend.text = element_text(family = "sans", size = 10)
  )
```


**Liveness**

Majority of the Liveness observations are below the 0.8 threshold which strongly suggests that the song may not be a live performance. There appears to be a concentration of data points with confidence measures ranging between 0.02 and 0.15 which begin at a song popularity score of 0. Thereafter, the measured observations increase and then relatively stagnate around 0.40, up to a song popularity score of 80. Of the observations with missing values, song popularity is concentrated around the region of 30 and 85. 


```{r missingness of Liveness by Song Popularity}

ggplot(data.cleaned, aes(x = song_popularity, y = Liveness)) +
  theme_minimal() +
  geom_miss_point() +
  labs(
    x = "Song Popularity",
    y = "Confidence Measure"
  ) +
  ggtitle("Liveness") +
  scale_y_continuous(
    breaks = seq(0, max(data.cleaned$Liveness, na.rm = TRUE), by = 0.10),
    labels = label_comma()
  ) +
  scale_x_continuous(
    breaks = seq(0, max(data.cleaned$song_popularity, na.rm = TRUE), by = 10),
    labels = label_comma()
  ) +
  guides(
    color = guide_legend(title = "Value Type", 
                         title.theme = element_text(family = "sans", 
                                                    face = "bold",
                                                    size = 10))
  ) + 
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black"),
    legend.text = element_text(family = "sans", size = 10)
  )
```


**Loudness**

The overall loudness of the observations appear to be concentrated between -15dB and 0 which is indicative of observations that are moderately to maximally calibrated. Of the observations with missing values, song popularity is concentrated around the scores that roughly lie from 34 to 83. 
 

```{r missingness of Loudness  by Song Popularity}
  
ggplot(data.cleaned, aes(x = song_popularity, y = Loudness)) +
  theme_minimal() +
  geom_miss_point() +
  labs(
    x = "Song Popularity",
    y = "Decibel (dB)"
  ) +
  ggtitle("Loudness") +
  scale_x_continuous(
    breaks = seq(0, max(data.cleaned$song_popularity, na.rm = TRUE), by = 10),
    labels = label_comma()
  ) +
  guides(
    color = guide_legend(title = "Value Type", 
                         title.theme = element_text(family = "sans", 
                                                    face = "bold",
                                                    size = 10))
  ) + 
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black"),
    legend.text = element_text(family = "sans", size = 10)
  )
```


*Speechiness*

There are evident outliers with confidence measures above 0.50 and have varying values along the x-axis. Majority of the observations have a song popularity score that ranges between 30 and 85. Of the observations with missing values, song popularity is concentrated around the scores that roughly lie from 40 to 82. 


```{r missingness of Speechiness by Song Popularity}
ggplot(data.cleaned, aes(x = song_popularity, y = Speechiness)) +
  theme_minimal() +
  geom_miss_point() +
  labs(
    x = "Song Popularity",
    y = "Confidence Measure"
  ) +
  ggtitle("Speechiness") +
  scale_y_continuous(
    breaks = seq(0, max(data.cleaned$Speechiness, na.rm = TRUE), by = 0.10),
    labels = label_comma()
  ) +
  scale_x_continuous(
    breaks = seq(0, max(data.cleaned$song_popularity, na.rm = TRUE), by = 10),
    labels = label_comma()
  ) +
  guides(
    color = guide_legend(title = "Value Type", 
                         title.theme = element_text(family = "sans", 
                                                    face = "bold",
                                                    size = 10))
  ) + 
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black"),
    legend.text = element_text(family = "sans", size = 10)
  )
```


**Tempo**

There are two distinct outliers with BPM values that are approximately 0 and 242. The other observations that marginally differ from majority of the data points are roughly 5-40 beats per minute greater than or less than the dominant share. The average beat duration of the sampled songs range between 80 and 190. Of the observations with missing values, song popularity is concentrated around the scores in the region of 40 to 80. 


```{r missingness of Tempo by Song Popularity}
  
ggplot(data.cleaned, aes(x = song_popularity, y = Tempo)) +
  theme_minimal() +
  geom_miss_point() +
  labs(
    x = "Song Popularity",
    y = "Tempo"
  ) +
  ggtitle("Tempo") +
  scale_y_continuous(
    breaks = seq(0, max(data.cleaned$Tempo, na.rm = TRUE), by = 20),
    labels = label_comma()
  ) +
  scale_x_continuous(
    breaks = seq(0, max(data.cleaned$song_popularity, na.rm = TRUE), by = 10),
    labels = label_comma()
  ) +
  guides(
    color = guide_legend(title = "Value Type", 
                         title.theme = element_text(family = "sans", 
                                                    face = "bold",
                                                    size = 10))
  ) + 
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black"),
    legend.text = element_text(family = "sans", size = 10)
  )
```


**Audio Valence**

The data set samples for the audio feature Audio Valence are randomly scattered across the x-axis and y-axis. Sparsely scattered data points are noticeable between the approximate song popularity scores of 0 and 30 as well as songs with a score roughly greater than 80. Of the observations with missing values, song popularity is concentrated around the scores that approximately fall between 37 and 78.


```{r missingness of Audio Valence by Song Popularity}
  
ggplot(data.cleaned, aes(x = song_popularity, y = Audio_valence)) +
  theme_minimal() +
  geom_miss_point() +
  labs(
    x = "Song Popularity",
    y = "Confidence Measure"
  ) +
  ggtitle("Audio Valence") +
  scale_x_continuous(
    breaks = seq(0, max(data.cleaned$song_popularity, na.rm = TRUE), by = 10),
    labels = label_comma()
  ) +
  scale_y_continuous(
    breaks = seq(0, max(data.cleaned$Audio_valence, na.rm = TRUE), by = 0.10),
    labels = label_comma()
  ) +
  guides(
    color = guide_legend(title = "Value Type", 
                         title.theme = element_text(family = "sans", 
                                                    face = "bold",
                                                    size = 10))
  ) + 
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black"),
    legend.text = element_text(family = "sans", size = 10)
  )
```


To view the distribution of the categorical variables Key, Audio mode and Time Signature, three box plots are visualised and a summary of the descriptive statistics are output as a matrix.

**Time Signature**

10,960 samples have four beats in a bar. None of the notational conventions have outliers moreover, the median across the time signatures are relatively similar. Songs with 4/4 and 5/4 notations have the same first and third quartile values. 

NMAR or MAR are the most likely missing data mechanisms as missing values may be distributed systematically owing to observations that have NA values across the audio feature Tempo. Tempo similarly accounts for track speed specifically the average beat duration of a song. However, as there are likely ample valid data points, missing values can be estimated (MAR).


```{r boxplot on time signature}
boxplot(song_popularity~Time_signature, 
        data = data.cleaned,
        col = "#66b2b2", 
        main = "Time Signature", xlab = "Time Signature",
        ylab = "Song Popularity")
```

```{r table time signature}
time.signature.stats <- boxplot(song_popularity ~ Time_signature, 
                    data = data.cleaned, 
                    plot = FALSE)

time.signature.stats
```


**Audio Mode**

The descriptive statistics on song modality are relatively similar. A difference of one distinguishes the first quartile figures and a difference of two contrasts the maximum values. There are 7,777 songs that have a melodic tone derived from a major scale which is 63.15 percent of the data set's samples. Approximately, for every 100 songs written using a minor scale, there are 171 tracks composed using a major scale.


```{r boxplot on audio mode}
boxplot(song_popularity~Audio_mode, data = data.cleaned, col = "#66b2b2", 
        main = "Audio Mode", xlab = "Audio Mode",
        ylab = "Song Popularity")
```

```{r table audio mode}
audio.mode.stats <- boxplot(song_popularity ~ Audio_mode, 
                    data = data.cleaned, 
                    plot = FALSE)

audio.mode.stats
```


**Key**

All keys have a relatively wide distribution of popularity scores, and the median value across the keys fall within a narrow range from 53 to 56. There are five keys with over 1,000 songs likely suggesting a composition preference among artists. Key 0 has the highest frequency which consists of 1,347 tracks. Key 3, 6, 8, 10 and 11 show clusters of low outliers. 

As all Audio Mode observations are exhaustive missing values in the Key column may likely be MAR. In addition to Audio Valence other attributes with valid data points could helpfully predict the correct Key. However, NMAR may likely be plausible as the inability of speech detection algorithms to signal the presence of spoken word in a song could affect the categorisation of a standard Pitch Class notation. 


```{r boxplot on key}
boxplot(song_popularity~Key, data = data.cleaned, col = "#66b2b2", 
        main = "Key", xlab = "Key",
        ylab = "Song Popularity")
```

```{r table Key}
key.stats <- boxplot(song_popularity ~ Key, 
                    data = data.cleaned, 
                    plot = FALSE)

key.stats
```


The vis_miss() plot exhibits a general pattern of missing values. The missing values are visibly scattered throughout the entire data matrix therefore, multiple imputation by chain equations (MICE) would suitably resolve missing values with the aforementioned configuration. The validity of statistical inferences would consequently be preserved (Enders 2022, p.2-3).

Predictor subgroups could potentially account for the missing observations. To mitigate modelling bias and enhance prediction precision, plausible data subsets of varying data types are computed iteratively and conditionally to best reflect existing relationships among the independent variables. Information from every variable is incorporated to formulate multivariate imputation by chained equations equations. 


```{r vis_miss plot across all features}

vis_miss(data.cleaned) +
  scale_fill_manual(values = c("black", "yellow"))

```


Removing observations that have at least three missing values is favorable. The extent of missing values across the 12 columns challenges the validity of data points that are present which will be drawn on to estimate and substitute missing values. Derived predictions from the MICE imputation may likely be laden with errors and biases which would consequently affect the generalisability of the results. Moreover, the inclusion of observations with non-exhaustive values may undermine the inherent variance of the original distribution. Listwise deletion may unlikely impact variable associations and interactions negatively, and subsequently distort song popularity predictions. A missing rate less than five percent is inconsequential (Schafer 1999).


```{r observations with NA values}
na.values <- data.cleaned[rowSums(is.na(data.cleaned)) >= 3, ]

paste0("There are ", nrow(na.values), " observations with at least three missing values which is ", round(nrow(na.values)/12316*100, 2), " percent of the data set.")
```

```{r missing values}
data.cleaned2 <- data.cleaned[!rowSums(is.na(data.cleaned)) >= 3, ]

paste0("Dataframe dimensions of the cleaned data updates are ", nrow(data.cleaned2), " rows and ", ncol(data.cleaned2), " columns.")
```


To prepare for MICE imputation, a method stored as a vetor is created to more accurately compute the predictions according to each variable's data type. 'pmm' is applied to integer and numeric variables, 'logreg' to binary variables and 'polyreg' to factor variables with more than two levels. Moreover, 30 multiple imputations and 30 iterations are calculated.


```{r data imputation}
data.imput <- data.cleaned2 |>
  select(-song_name)

method.vector <- c(Song_popularity = "pmm",
                   Duration_ms = "pmm",
                   Acousticness = "pmm",
                   Danceability = "pmm",
                   Energy = "pmm",
                   Instrumentalness = "pmm",
                   Key = "polyreg",
                   Liveness = "pmm",
                   Loudness = "pmm",
                   Audio_mode = "logreg",
                   Speechiness = "pmm",
                   Tempo = "pmm",
                   Time_signature = "polyreg",
                   Audio_valence = "pmm")

mice.imputation <- mice(data = data.imput,
                        method = method.vector,
                        m = 30,
                        maxit = 30,
                        seed = 1234)
```


To evaluate the imputed values a stripplot is visualised. The imputed values, drawn from observed data points that closely match randomly selected donor candidates, are reasonably well distributed across the explanatory variables with missing data points. The blue dots indicate that the data points are from the original data set while the red dots are illustrative of the imputed values. 


```{r strip plot}
stripplot(mice.imputation,
pch=16)
```


As there are 30 imputed data sets with 30 iterations each, visually inspecting a convergence plot as well as assessing regression model pooled statistics and assumptions would likely strengthen interpretations on the validity and reliability of the imputed values. 

Evidence of weaving is exhibited with no identifiable trend; the samples oscillate between the narrow parameter ranges (mean and standard deviation) suggesting that there is limited drift or volatility over the iterations. A sufficient number of iterations were formulated in order for the algorithm to settle into a stable imputation distribution. 

To further assess the performance of the selected imputation technique, a pooled adjusted r-squared estimate, confidence interval and fraction of missing information are useful metrics to view. 


```{r convergence plots 1}
plot(mice.imputation,
     lwd = 2)
```


The pooled adjusted R² from the regression models is 4.15%, with a 95% confidence interval ranging from 3.46% to 4.90%. The fraction of missing information (fmi) is 6.10%%. As the data is quite complex, a low adjusted R² statistic is not irregular. Although the predictors limitedly explain variance in song popularity, the confidence interval is relatively narrow which suggests that the estimate is fairly precise. Furthermore, the impact of the missing data does not significantly affect the predictive power of the regression model as the percentage is less than 10 percent. To predict song popularity an explanatory model that accounts for the data set's uncertainty is preferential. 


```{r regression model statistics}
reg.models.mice <- with(mice.imputation,
                      lm(song_popularity ~ Duration_ms + Acousticness + Danceability + Energy + 
                           Instrumentalness + Key + Liveness + Loudness + Audio_mode + Speechiness + 
                           Tempo + Time_signature + Audio_valence))

pool.r2 <- pool.r.squared(reg.models.mice, adjusted = TRUE)

pool.r2
```


To select the imputed data set for model building and song popularity predictions, the individual adjusted r-squared values are arranged and output to identify the regression model that would most optimally compute predictions. Thereafter, residual plots are created to examine the identified model's regression assumptions and Variation Inflation Factor (VIF) values are output to assess multicolleniarty. 


```{r distinct adjusted r-squared values}
adjr2 <- numeric(mice.imputation$m)

for (i in 1:mice.imputation$m) {
  imputation.data <- complete(mice.imputation, action = i)
  
  model <- lm(song_popularity ~ Duration_ms + Acousticness + Danceability + Energy + 
                Instrumentalness + Key + Liveness + Loudness + Audio_mode + Speechiness + 
                Tempo + Time_signature + Audio_valence, data = imputation.data)
  
  adjr2[i] <- summary(model)$adj.r.squared
}

adjr2.df <- as.data.frame(adjr2) 

adjr2.df$Imputation <- c(1:15)

adjr2.df <- adjr2.df |>
  group_by(adjr2, Imputation) |>
  arrange(desc(adjr2)) |>
  rename(`Adjusted R-Squared` = adjr2) 

kable(adjr2.df)
```



The residuals appear to be clustered across the Residuals vs Fitted, Scale-Location and Residuals vs Leverage plots therefore constant variance is unlikely. Considering the size of the data set, the distribution of values above and below the mean are relatively proportionate which suggests that the constant mean of zero assumption may uphold. However, as the residuals deviate from the diagonal line of the Q-Q plot, the values are unlikely normally distributed. 


```{r regression assumptions imputation 10}
par(mfrow=c(2,2))

plot(reg.models.mice$analyses[[5]], pch=16)
```


Evaluating VIF values for each independent variable adequately determines the presence of multicollinearity. A VIF value greater than 5 is indicative of high  multicollinearity whereas values that are greater than 1 and less than or equal to 5 suggest a moderate correlation with other covariates. 

The VIF results show no evidence of problematic multicollinearity. All the adjusted GVIF values are below 2 therefore the existing correlations are not enough to distort model estimation. 


```{r multicolliniarty check}
vif(reg.models.mice$analyses[[5]])
```

All the missing values have been resolved.

```{r dataset imputation 10}
data.cleaned3 <- complete(mice.imputation, action = 5)

kable(miss_var_summary(data.cleaned3))
```


The MICE imputation has a consistent distribution shape with the original distribution. 


```{r distribution comparison original vs MICE computation}

p1 <- ggplot(data.cleaned, aes(song_popularity))+
  geom_histogram(fill = "#CC5500", color = "#4dffc0")+
  theme_minimal() +
  labs(
    x = "Song Popularity",
    y = "Count"
    ) +
  ggtitle("Original Distribution of Song Popularity") +
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black")
  )
p2 <- ggplot(data.cleaned3, aes(song_popularity)) +
  geom_histogram(fill = "#5500cc", color = "#4dffc0") +
  theme_minimal() +
  labs(
    x = "Song Popularity",
    y = "Count"
  ) +
  ggtitle("MICE imputed values of Song Popularity") +
  theme(
    plot.title = element_text(family = "sans", face = "bold", hjust = 0.5, size = 14, colour = "black"),
    axis.title.x = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.title.y = element_text(family = "sans", face = "bold", size = 10, colour = "black"),
    axis.text = element_text(family = "sans", hjust = 0.5, size = 9, face = "bold", colour = "black")
  )
plot_grid(p1, p2, nrow = 2, ncol = 1)

```


#### Artificial Neural Network Model Building

Train and test sets are split into two, and stratified sampling is used as a response variable.


```{r train-test split}
set.seed(202454419)

song.pop.split <- initial_split(
  data.cleaned3, 
  prop = 0.8, 
  strata = song_popularity
  )

song.pop.train <- training(song.pop.split)

song.pop.test <- testing(song.pop.split)
```


A 5-fold cross validation is created for parameter tuning.


```{r cross validation}
cross.val.folds <- vfold_cv(song.pop.train, 
                            v = 5,
                            strata = song_popularity)
```


An Artificial Neural Network (ANN) model expects numeric variables. Input data are processed as vectors of numbers. Therefore, the predictors Key, Time Signature and Audio Mode ought to be transformed to numeric variables as factors do not have an inherent numerical meaning. 

To address skewed numeric data, YeoJohnson transformation is included in the ANN recipe to help form a symmetric distribution. Moreover, to better prepare the data for ANN modelling all the explanatory variables need to be normalised. Normalisation ensures that each independent variable equally contributes to model performance by preventing bias towards variables with a larger magnitude. Moreover, the convergence speed during training is improved. 

The selected gradient optimisation algorithm to minimise the loss function through consistent penalties of large parameters controls the likelihood of overfitting. Similarly, the likelihood of an underfitting model due to vanishing gradients is stabalised. 


```{r ANN recipe}
nnet.recipe <- recipe(song_popularity ~ Duration_ms + Acousticness + Danceability + Energy + 
                          Instrumentalness + Key + Liveness + Loudness + Audio_mode + Speechiness + 
                          Tempo + Time_signature + Audio_valence, data = song.pop.train) |> 
  step_YeoJohnson(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_predictors())
```


The ANN model specifications are an nnet engine and the hyperparameters are hidden units, penalty and epochs.


```{r ANN model, nnet engine and hyperparameters}
nnet.spec <- mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) |>
  set_engine("nnet", MaxNWts = 2600) |>
  set_mode("regression")
```

```{r ANN workflow}
nnet.wflow <-
  workflow() |>
  add_model(nnet.spec) |>
  add_recipe(nnet.recipe)
```


The model metrics selected for parameter tuning include residual mean squared error (RMSE) and adjusted r-squared (R²). RMSE captures the difference between a predicted value and true value while R² is indicative of the extent to which the explanatory variables account for influences on a response variable. Both values are useful for measuring the predictive power of a model. 


```{r model metrics}
mod.metrics <- metric_set(rmse, rsq)
```


To tune the ANN model's three parameters, racing method and 40 grid points are utlisied.


```{r racing method}
system.time(nnet.race <- nnet.wflow |>
              tune_race_anova(
                cross.val.folds,
                grid = 40,
                metrics = mod.metrics,
                control = control_race(verbose_elim = TRUE)
              )
)
```


The combination of model parameters with the best residual mean squared error have 1,2 or 6 hidden units, penalty terms that range between 0.00000002030918 and 0.05223345, and epoch values that vary from as few as 10 to as many as 695.


```{r best hyperparameters rmse}
show_best(nnet.race, metric = "rmse")|>
  select(-.estimator, -n, -.config, -.metric)
```


A visualisation of relative feature importance in predicting song popularity shows that the three pivotal predictors are Loudness, Instrumentalness and Energy . 


```{r optmised workflow, model fit and predictions}
best.rmse <- nnet.race |> 
  select_best(metric = "rmse")

final.nnet.wflow <- nnet.wflow |>
  finalize_workflow(best.rmse) 
 
final.nnet.fit <- final.nnet.wflow |>
  fit(data.cleaned3)

explainer.reg <- explain_tidymodels(
  final.nnet.fit,
  data = data.cleaned3 |> 
    select(-song_popularity), 
  y = data.cleaned3 |> 
    select(song_popularity),
    label = "Regression",
    verbose = FALSE
  )

model.feat.reg <- model_parts(explainer.reg)
plot(model.feat.reg)
```


Partial Dependency profiles (PDPs) and Accumulated Local Effects (ALE) offer valuable insights into how various music production features influence the popularity of a song. ALE in particular exhibits more robust profiles as nearest neighbours are accounted for which consider the effect of multicolleniarity. Although the consequences of correlation on song popularity predictions are marginal as noted by the computed VIF values, relative changes over a normalised scale are considered. 

Based on the ANN algoritm, an increase in a track's energy which reflects a songs intensity and activity, is associated with a steady decline in popularity. This suggests that lower energy songs may be more frequently streamed on Spotify. Similarly, as the instrumentalness of a song increases, song popularity initially dips and minimally declines thereafter. There is a clear distinction regarding vocal preference. Listeners likely engage with music that has clear vocal content. As audible vocals taper song popularity songs minimally diverge. In contrast, loudness appears to have a positive relationship with a track's popularity. As loudness increases, song popularity equally increases, with a notable rise around -15 dB. This may indicate the presence of an amplitude “sweet spot” that appeals to keen listeners.


```{r partial effects of vip features}
pdp <- model_profile(
  explainer.reg,
  N = 100,
  variables = c("Energy", "Loudness", "Instrumentalness", "Danceability", "Audio_valence")
)

par(mfrow = c(1, 1))
plot(pdp) +
  ggtitle("Partial local profiles")

```

```{r accumulated local effects of vip features}
adp <- model_profile(
  explainer.reg,
  type = "accumulated",
  N = 100,
  variables = c("Energy", "Loudness", "Instrumentalness", "Danceability", "Audio_valence")
)

plot(adp) +
  ggtitle("Accumulated local effects")
```
